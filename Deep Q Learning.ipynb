{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import sys\n","\n","drive.mount('/content/drive');\n","\n","sys.path.append('/content/drive/My Drive/Projects/TTT/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MO_W0jpXBQYV","executionInfo":{"status":"ok","timestamp":1692127155453,"user_tz":240,"elapsed":19380,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}},"outputId":"aafaa111-66a9-4294-83a8-83b2cb962118"},"id":"MO_W0jpXBQYV","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from game import TicTacToe"],"metadata":{"id":"O85nHv07CBlz","executionInfo":{"status":"ok","timestamp":1692127158582,"user_tz":240,"elapsed":3133,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"id":"O85nHv07CBlz","execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":17,"id":"4c01bf44","metadata":{"id":"4c01bf44","executionInfo":{"status":"ok","timestamp":1692128199933,"user_tz":240,"elapsed":168,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"outputs":[],"source":["import math\n","import random\n","import pickle as pkl\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import Dataset"]},{"cell_type":"markdown","source":["### **Training data**\n","Transition namedtuples from past play\n"],"metadata":{"id":"0TpgHeLR-B-9"},"id":"0TpgHeLR-B-9"},{"cell_type":"code","source":["Transition = namedtuple('Transition',\n","                        ('state', 'action', 'next_state', 'reward'))\n","\n","class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def push(self, *args):\n","        \"\"\"Save a transition\"\"\"\n","        self.memory.append(Transition(*args))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n"],"metadata":{"id":"HUi7cgIsMOm1","executionInfo":{"status":"ok","timestamp":1692127162914,"user_tz":240,"elapsed":6,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"id":"HUi7cgIsMOm1","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### **Define network**"],"metadata":{"id":"pl2znJTR-Y2O"},"id":"pl2znJTR-Y2O"},{"cell_type":"code","source":["class DQN(nn.Module):\n","    '''\n","        n_observations : number of state observations\n","        n_actions : reward for actions\n","    '''\n","\n","    def __init__(self, n_observations, n_actions):\n","\n","        self.features_A = 64\n","\n","        super(DQN, self).__init__()\n","        self.layer1 = nn.Linear(n_observations, self.features_A)\n","        self.layer2 = nn.Linear(self.features_A, self.features_A)\n","        #self.layer3 = nn.Linear(self.features_A, self.features_A)\n","        self.layer4 = nn.Linear(self.features_A, n_actions)\n","\n","    # Called with either one element to determine next action, or a batch\n","    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n","\n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        #x = F.relu(self.layer3(x))\n","        return self.layer4(x)"],"metadata":{"id":"lgTLSsX6_dFS","executionInfo":{"status":"ok","timestamp":1692127162915,"user_tz":240,"elapsed":5,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"id":"lgTLSsX6_dFS","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### **Hyperparameters and  Utilities**"],"metadata":{"id":"iqmFpiOkDkoE"},"id":"iqmFpiOkDkoE"},{"cell_type":"code","source":["# BATCH_SIZE is the number of transitions sampled from the replay buffer\n","# GAMMA is the discount factor as mentioned in the previous section\n","# EPS_START is the starting value of epsilon\n","# EPS_END is the final value of epsilon\n","# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n","# TAU is the update rate of the target network\n","# LR is the learning rate of the ``AdamW`` optimizer\n","BATCH_SIZE = 128\n","GAMMA = 0.55\n","TAU = 0.005\n","LR = 1e-4\n","\n","n_actions = 9\n","n_observations = 9\n","\n","# if GPU is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","policy_net = DQN(n_observations, n_actions).to(device)\n","target_net = DQN(n_observations, n_actions).to(device)\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)"],"metadata":{"id":"RZlWiHkCC-Ng","executionInfo":{"status":"ok","timestamp":1692127168127,"user_tz":240,"elapsed":5217,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"id":"RZlWiHkCC-Ng","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### **Helper tools**"],"metadata":{"id":"ajSoZ9kard1p"},"id":"ajSoZ9kard1p"},{"cell_type":"code","source":["def one_hot_and_tensify(position):\n","    one_hot = [0 if x != position else 1 for x in range(9)]\n","    return torch.tensor(one_hot, dtype=torch.int64, device=device)\n","\n","def make_reward_tensor(reward):\n","    #one_hot = [0 if x != position else reward for x in range(9)]\n","    return torch.tensor(reward, dtype = torch.int64, device=device)\n","\n","class MemoryDataset(Dataset):\n","    def __init__(self, raw_memory):\n","        self.raw_memory = raw_memory\n","\n","    def __len__(self):\n","        return len(self.raw_memory)\n","\n","    def __getitem__(self, idx):\n","        entry = self.raw_memory[idx]\n","\n","        state = torch.tensor(list(entry[0]), dtype=torch.float32)\n","\n","        action = one_hot_and_tensify(entry[1])\n","\n","        next_state = torch.tensor(list(entry[2]), dtype=torch.float32)\n","\n","        reward = make_reward_tensor(entry[3])\n","        return state, action, next_state, reward"],"metadata":{"id":"7guS63VsrifS","executionInfo":{"status":"ok","timestamp":1692128206166,"user_tz":240,"elapsed":149,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"id":"7guS63VsrifS","execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### **Format incoming data for model**\n","\n","- (state, action, next state, reward) -> (tensor, tensor, tensor, int)"],"metadata":{"id":"1gkQKlvpccpp"},"id":"1gkQKlvpccpp"},{"cell_type":"code","source":["with open('drive/My Drive/Projects/TTT/data/memory_0.pkl', 'rb') as file:\n","    raw_train_memory = pkl.load(file)\n","\n","print(f'Number of (state, action, next_state,  reward) items is {len(raw_memory)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPTLdV8q8b1s","executionInfo":{"status":"ok","timestamp":1692128209200,"user_tz":240,"elapsed":565,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}},"outputId":"f30a9422-afa5-4e21-c453-67dedcbee6b5"},"id":"kPTLdV8q8b1s","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of (state, action, next_state,  reward) items is 711130\n"]}]},{"cell_type":"markdown","source":["#### **Sandbox to troubleshoot mask issues**"],"metadata":{"id":"1AbWaEAPvUSq"},"id":"1AbWaEAPvUSq"},{"cell_type":"code","source":["# training dataloader\n","dataset = MemoryDataset(raw_train_memory)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","i = 0\n","rounds = 5\n","\n","for state_batch, action_batch, next_state_batch, reward_batch in dataloader:\n","        i += 1\n","        if i == rounds : break\n","\n","        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next_state_batch)), device=device, dtype=torch.bool)\n","        non_final_next_states = torch.stack([s for s in next_state_batch if s is not None], dim = 0)\n","\n","        golden_dim = state_batch.shape[0]\n","\n","        print(f'dimension = {golden_dim}\\tstate_batch shape {state_batch.shape}\\t mask shape {non_final_mask.shape}\\t next_states shape {non_final_next_states.shape}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PhNH5_SvQOU","executionInfo":{"status":"ok","timestamp":1692129823797,"user_tz":240,"elapsed":486,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}},"outputId":"1fceaa16-df63-45bf-9d8e-600f3e8c3e28"},"id":"2PhNH5_SvQOU","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["dimension = 128\tstate_batch shape torch.Size([128, 9])\t mask shape torch.Size([128])\t next_states shape torch.Size([128, 9])\n","dimension = 128\tstate_batch shape torch.Size([128, 9])\t mask shape torch.Size([128])\t next_states shape torch.Size([128, 9])\n","dimension = 128\tstate_batch shape torch.Size([128, 9])\t mask shape torch.Size([128])\t next_states shape torch.Size([128, 9])\n","dimension = 128\tstate_batch shape torch.Size([128, 9])\t mask shape torch.Size([128])\t next_states shape torch.Size([128, 9])\n"]}]},{"cell_type":"markdown","source":["## **Training**\n"],"metadata":{"id":"wg1ME_n7lM09"},"id":"wg1ME_n7lM09"},{"cell_type":"code","source":["# training dataloader\n","dataset = MemoryDataset(raw_train_memory)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","for state_batch, action_batch, next_state_batch, reward_batch in dataloader:\n","\n","    ## update 2nd model?\n","    state_batch = state_batch.to(device)\n","    action_batch = action_batch.to(device)\n","    next_state_batch = next_state_batch.to(device)\n","    reward_batch = reward_batch.to(device)\n","\n","    # Compute a mask of non-final states\n","    # (a final state's next state would've been after the simulation ended)\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next_state_batch)), device=device, dtype=torch.bool)\n","    non_final_next_states = torch.stack([s for s in next_state_batch if s is not None], dim = 0)\n","\n","    actions_taken = action_batch.argmax(dim=1, keepdim=True)\n","\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","    # columns of actions taken. These are the actions which would've been taken\n","    # for each batch state according to policy_net\n","    ''' This is the forward pass. '''\n","    state_action_values = policy_net(state_batch).gather(1, actions_taken)\n","\n","\n","    # Compute V(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","\n","\n","    current_batch_size = state_batch.shape[0]\n","    next_state_values = torch.zeros(current_batch_size, device=device)\n","\n","    ''' Next states values calculated '''\n","    with torch.no_grad():\n","        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n","\n","    # Compute the expected Q values\n","    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","\n","    state_action_values = state_action_values.squeeze(1)\n","\n","    # Compute Huber loss\n","    criterion = nn.SmoothL1Loss()\n","    loss = criterion(state_action_values, expected_state_action_values)\n","\n","    #print(f'Loss is {loss}')\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # In-place gradient clipping\n","    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n","    optimizer.step()"],"metadata":{"id":"9BbB_fH_lsFe","executionInfo":{"status":"ok","timestamp":1692129926507,"user_tz":240,"elapsed":59116,"user":{"displayName":"joseph hewitt","userId":"12680800915260642796"}}},"id":"9BbB_fH_lsFe","execution_count":27,"outputs":[]},{"cell_type":"code","source":["def pick_from_DQN(player, board, *, validate_move = False):\n","\n","    if player == 'o' :\n","        board = [x * -1 for x in board]\n","\n","    state = torch.tensor(board, dtype=torch.float32, device=device)\n","\n","    with torch.no_grad():\n","        # pass the state through the network\n","        q_values = policy_net(state)\n","\n","    if validate_move :\n","        # select first valid action with  the  highest Q-value\n","        for index in list(torch.argsort(q_values, descending = True)):\n","            if state[index] == 0:\n","                action = index\n","                break\n","\n","    else:\n","        # select the action with the highest Q-value\n","        action = torch.argmax(q_values).item()\n","\n","    return int(action)"],"metadata":{"id":"UeT3wEelQ_kG"},"id":"UeT3wEelQ_kG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from game import train_Q_table\n","from game import pick_from_greedy_heuristic\n","from game import pick_from_Q_table\n","from game import pick_from_random"],"metadata":{"id":"r4n1O3i_axyv"},"id":"r4n1O3i_axyv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Train Q Learning Table**"],"metadata":{"id":"AdBooe85bSHD"},"id":"AdBooe85bSHD"},{"cell_type":"code","source":["train_q = False\n","\n","if train_q:\n","    competitor_policy_training = 'greedy'\n","    training_rounds = 50000\n","\n","    q_learning_rate = 0.3\n","    q_discount_factor = 0.55\n","\n","    q_table, memory = train_Q_table(rounds = training_rounds,\n","                                        learning_rate = q_learning_rate,\n","                                        discount_factor = q_discount_factor,\n","                                        competitor_policy = competitor_policy_training)\n","\n","    print(f'Q Table training Information :')\n","    print(f'\\tTraining rounds : {training_rounds}')\n","    print(f'\\tCompetitor training policy : {competitor_policy_training}')\n","    print(f'\\n\\tQ Learning Rate : {q_learning_rate}')\n","    print(f'\\tQ Discount Factor : {q_discount_factor}')"],"metadata":{"id":"EAD-iBtla7zl"},"id":"EAD-iBtla7zl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Run Trial**"],"metadata":{"id":"GYFNRpzPavtR"},"id":"GYFNRpzPavtR"},{"cell_type":"code","source":["trial_rounds = 5000"],"metadata":{"id":"-nKUCD-pbMwq"},"id":"-nKUCD-pbMwq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["stats = {\n","    1 : {\n","            'h' : 0,\n","            'v' : 0,\n","            'd' : 0,\n","            'win_lengths' : [0 for x in range(10)]\n","        },\n","    -1 : {\n","            'h' : 0,\n","            'v' : 0,\n","            'd' : 0,\n","            'win_lengths' : [0 for x in range(10)]\n","        },\n","    -2 : 0\n","}\n","\n","for i in range(trial_rounds) :\n","\n","    # alternate who goes first\n","    first_move_x = (i%2 == 0)\n","    ttt = TicTacToe(first_move_x = first_move_x)\n","\n","    while ttt.win == False and ttt.age < 9 :\n","        if ttt.turn == 1 :\n","            player = 'x'\n","\n","            # Policy Selection\n","            #position = pick_from_random(ttt.board)\n","            #position = pick_from_greedy_heuristic(player, ttt.board)\n","            #position = pick_from_Q_table(player, q_table, ttt.board)\n","            position = pick_from_DQN(player, ttt.board, validate_move = True)\n","\n","        else :\n","            player = 'o'\n","\n","            # Policy Selection\n","            position = pick_from_random(ttt.board)\n","            #position = pick_from_greedy_heuristic(player, ttt.board)\n","            #position = pick_from_Q_table(player, q_table, ttt.board)\n","            #position = pick_from_DQN(player, ttt.board, validate_move = True)\n","\n","        result = ttt.move(player, position)\n","        assert(result == 100)\n","\n","    # keep track of stats\n","    if ttt.winner == -2 :\n","        stats[ttt.winner] += 1\n","    else:\n","        stats[ttt.winner][ttt.win_dir] += 1\n","        stats[ttt.winner]['win_lengths'][ttt.age] += 1\n"],"metadata":{"id":"wODGq4pFbjeV"},"id":"wODGq4pFbjeV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["stats[1]['total'] = sum([x for x in list(stats[1].values()) if isinstance(x, int)])\n","stats[-1]['total'] = sum([x for x in list(stats[-1].values()) if isinstance(x, int)])"],"metadata":{"id":"StoZsuyJWc_L"},"id":"StoZsuyJWc_L","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'\\n\\tTrial Rounds : {trial_rounds}')\n","print(f'\\tPlayer 1 wins : {stats[1][\"total\"]}\\t ({stats[1][\"total\"] / trial_rounds : .3})\\t {stats[1][\"win_lengths\"]}')\n","print(f'\\tPlayer -1 wins : {stats[-1][\"total\"]}\\t ({stats[-1][\"total\"] / trial_rounds : .3})\\t {stats[-1][\"win_lengths\"]})')\n","print(f'\\tDraws : {stats[-2]}\\t\\t ({stats[-2] / trial_rounds : .3})')"],"metadata":{"id":"kf3-ERsNbyY_"},"id":"kf3-ERsNbyY_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(policy_net, 'drive/My Drive/Projects/TTT/model.pth')\n","torch.save(policy_net, 'drive/My Drive/Projects/TTT/model_weights.pth')"],"metadata":{"id":"XJ9kYAW_USUD"},"id":"XJ9kYAW_USUD","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}